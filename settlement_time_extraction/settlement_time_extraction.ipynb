{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6D1WjimidLH",
        "outputId": "d7685835-c7c0-41b4-ba19-52bb19b46a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.2.1)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "pages_to_extract = [1, 2]\n",
        "all_data_rows = []\n",
        "\n",
        "date_pattern = re.compile(r'^(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday)\\s+\\d{1,2}\\s+[A-Za-z]+', re.IGNORECASE)\n",
        "\n",
        "expected_cols = 6\n",
        "\n",
        "with pdfplumber.open('/content/Trading_Schedule.pdf') as f:\n",
        "    for page_num in pages_to_extract:\n",
        "        page = f.pages[page_num]\n",
        "        tables = page.extract_tables()\n",
        "\n",
        "        if tables:\n",
        "            for table in tables:\n",
        "                for row in table:\n",
        "                    if row and row[0] and date_pattern.match(row[0]):\n",
        "\n",
        "                        if len(row) < expected_cols:\n",
        "                            row.extend([''] * (expected_cols - len(row)))\n",
        "\n",
        "                        if page_num > 1:\n",
        "\n",
        "                            date = row[0]\n",
        "                            col1 = row[1]\n",
        "                            col2 = row[2]\n",
        "                            col6 = row[3]\n",
        "\n",
        "\n",
        "                            row = [\n",
        "                                date,\n",
        "                                '',\n",
        "                                '',\n",
        "                                col1,\n",
        "                                col2,\n",
        "                                col6\n",
        "                            ]\n",
        "\n",
        "                        all_data_rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(all_data_rows, columns=['Date', 'Col2', 'Col3', 'Col4', 'Col5', 'Col6'])\n",
        "df.to_csv('x.csv', index=False)\n",
        "print(\"saved to x.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mPVtXoAigG5",
        "outputId": "a4a0021e-e4b2-419d-e597-56986f75dd73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved to x.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "def parse_date_dynamic(date_str):\n",
        "    try:\n",
        "        return pd.to_datetime(date_str, format=\"%A %d %B %Y\").strftime(\"%d/%m/%Y\")\n",
        "    except:\n",
        "        try:\n",
        "            match = re.match(r\"(\\w+ \\d{1,2} \\w+)\", date_str)\n",
        "            if match:\n",
        "                date_without_year = match.group(1)\n",
        "                for year in range(2024, 2030):\n",
        "                    full_date = f\"{date_without_year} {year}\"\n",
        "                    try:\n",
        "                        parsed = pd.to_datetime(full_date, format=\"%A %d %B %Y\")\n",
        "\n",
        "                        if parsed.strftime(\"%A %d %B\") == date_without_year:\n",
        "                            return parsed.strftime(\"%d/%m/%Y\")\n",
        "                    except:\n",
        "                        continue\n",
        "        except:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "df_full = pd.read_csv(\"x.csv\", header=None)\n",
        "\n",
        "df_full = df_full.drop([1, 2], axis=1)\n",
        "\n",
        "df_trimmed = df_full.iloc[4:].reset_index(drop=True)\n",
        "df_trimmed.columns = ['Date', 'Occasion', 'Status', 'Additional Information']\n",
        "\n",
        "valid_rows = []\n",
        "for _, row in df_trimmed.iterrows():\n",
        "    if pd.isna(row['Date']) or str(row['Date']).strip() == \"\":\n",
        "        break\n",
        "    valid_rows.append(row)\n",
        "\n",
        "df_cleaned = pd.DataFrame(valid_rows).reset_index(drop=True)\n",
        "\n",
        "df_cleaned['Date'] = df_cleaned['Date'].apply(parse_date_dynamic)\n",
        "\n",
        "print(df_cleaned.head())\n",
        "\n",
        "df_cleaned.to_csv(\"cleaned_data.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xx2LcG7o8bM_",
        "outputId": "146c226a-3f04-4649-f9e9-2af0ed3d2349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                        Occasion  Status  \\\n",
            "0  17/02/2025      Presidents Day\\nUS Holiday       A   \n",
            "1  28/03/2025                 Hari Raya Puasa      TD   \n",
            "2  17/04/2025                 Easter Thursday      TD   \n",
            "3  18/04/2025  Good Friday\\nUK and US Holiday  Closed   \n",
            "4  21/04/2025       Easter Monday\\nUK Holiday      TD   \n",
            "\n",
            "                              Additional Information  \n",
            "0  For further details, please refer to Trading S...  \n",
            "1  Singapore Markers & Transitioned Oil TAPS 04:2...  \n",
            "2  Singapore Markers & Transitioned Oil TAPS 05:2...  \n",
            "3                                                NaN  \n",
            "4  IFEU Agricultural Commodities Closed\\nUK Natur...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "import json\n",
        "import re\n",
        "\n",
        "def extract_trading_schedule_key(pdf_path):\n",
        "    # Initialize the dictionary to store the extracted data\n",
        "    trading_schedule = {}\n",
        "\n",
        "    # Define the keys to extract\n",
        "    keys = ['A', 'B', 'C', 'D']\n",
        "\n",
        "    try:\n",
        "        # Open the PDF file\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            print(f\"PDF loaded successfully. Total pages: {len(pdf.pages)}\")\n",
        "\n",
        "            # Target pages 4 and 5 where TRADING SCHEDULE KEY is located\n",
        "            target_pages = [3, 4]  # Page 4 (index 3) and Page 5 (index 4)\n",
        "            full_text = \"\"\n",
        "\n",
        "            # Extract text from target pages\n",
        "            for page_num in target_pages:\n",
        "                page = pdf.pages[page_num]\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    full_text += text + \"\\n\"\n",
        "                    print(f\"\\n--- Page {page_num + 1} Text Preview ---\\n{text[:200]}...\\n\")\n",
        "                else:\n",
        "                    print(f\"Page {page_num + 1}: No text extracted.\")\n",
        "\n",
        "            # Split the combined text into lines\n",
        "            lines = full_text.split('\\n')\n",
        "\n",
        "            current_key = None\n",
        "            current_content = []\n",
        "            in_schedule_key = False\n",
        "\n",
        "            # Regex to match keys at the start of a line\n",
        "            key_pattern = re.compile(r'^\\s*([ABCD])(?:\\s+|$)', re.MULTILINE)\n",
        "\n",
        "            for i, line in enumerate(lines):\n",
        "                # Skip empty lines or irrelevant headers/footers\n",
        "                if not line.strip() or line.startswith('IFEU Trading Schedule') or line.startswith('© 2025') or line.startswith('Q2005'):\n",
        "                    continue\n",
        "\n",
        "                # Detect start of TRADING SCHEDULE KEY section\n",
        "                if 'TRADING SCHEDULE KEY' in line:\n",
        "                    in_schedule_key = True\n",
        "                    print(\"Found TRADING SCHEDULE KEY section\")\n",
        "                    continue\n",
        "\n",
        "                # Only process lines within the TRADING SCHEDULE KEY section\n",
        "                if in_schedule_key:\n",
        "                    # Check if the line starts with a key (A, B, C, D)\n",
        "                    match = key_pattern.match(line)\n",
        "                    if match:\n",
        "                        key = match.group(1)\n",
        "                        # If we were collecting content for a previous key, save it\n",
        "                        if current_key:\n",
        "                            trading_schedule[current_key] = '\\n'.join(current_content).strip()\n",
        "                            print(f\"Extracted key {current_key} with content: {trading_schedule[current_key][:100]}...\")\n",
        "                        # Start collecting for the new key\n",
        "                        current_key = key\n",
        "                        current_content = [line[len(key):].strip()]  # Include the rest of the line after the key\n",
        "                        print(f\"Found key {current_key} at line: {line}\")\n",
        "                    elif current_key:\n",
        "                        # Add line to the current key's content\n",
        "                        current_content.append(line.strip())\n",
        "\n",
        "            # Save the last key's content\n",
        "            if current_key and current_content:\n",
        "                trading_schedule[current_key] = '\\n'.join(current_content).strip()\n",
        "                print(f\"Extracted key {current_key} with content: {trading_schedule[current_key][:100]}...\")\n",
        "\n",
        "            if not trading_schedule:\n",
        "                print(\"No keys (A, B, C, D) found in the TRADING SCHEDULE KEY section.\")\n",
        "            else:\n",
        "                print(f\"Extracted keys: {list(trading_schedule.keys())}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{pdf_path}' was not found. Ensure it is in the same directory as the script.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {str(e)}\")\n",
        "\n",
        "    return trading_schedule\n",
        "\n",
        "def save_to_json(data, output_path):\n",
        "    # Save the extracted data to a JSON file\n",
        "    with open(output_path, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "    print(f\"Data saved to {output_path}\")\n",
        "\n",
        "def main():\n",
        "    pdf_path = \"Trading_Schedule.pdf\"\n",
        "    output_path = \"trading_schedule_key.json\"\n",
        "\n",
        "    # Extract the trading schedule key data\n",
        "    trading_schedule = extract_trading_schedule_key(pdf_path)\n",
        "\n",
        "    # Save the extracted data to a JSON file if not empty\n",
        "    if trading_schedule:\n",
        "        save_to_json(trading_schedule, output_path)\n",
        "    else:\n",
        "        print(\"No data extracted to save.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtrSjoA3WUbi",
        "outputId": "366ee158-7bbf-4004-fa12-0051387cb8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF loaded successfully. Total pages: 10\n",
            "\n",
            "--- Page 4 Text Preview ---\n",
            "TRADING SCHEDULE KEY\n",
            "TD\n",
            "Regular Trading Hours and Settlement Times\n",
            "(Trading Day)\n",
            "A Trading Hours\n",
            "Regular Trading Hours EXCEPT for the following contracts:\n",
            "White Sugar Futures & Options early close at ...\n",
            "\n",
            "\n",
            "--- Page 5 Text Preview ---\n",
            "C Trading Hours\n",
            "Early close at 18:30 UK (13:30 ET)\n",
            "Brent, WTI, Midland WTI AGC and Permian WTI Storage Futures & Options\n",
            "Low Sulphur Gasoil Futures & Options\n",
            "Dubai Crude Futures & Options\n",
            "Heating Oil,...\n",
            "\n",
            "Found TRADING SCHEDULE KEY section\n",
            "Found key A at line: A Trading Hours\n",
            "Extracted key A with content: Trading Hours\n",
            "Regular Trading Hours EXCEPT for the following contracts:\n",
            "White Sugar Futures & Option...\n",
            "Found key B at line: B Trading Hours\n",
            "Extracted key B with content: Trading Hours\n",
            "Early close at 20:00 UK (15:00 ET)\n",
            "Brent, WTI, Midland WTI AGC and Permian WTI Storage...\n",
            "Found key C at line: C Trading Hours\n",
            "Extracted key C with content: Trading Hours\n",
            "Early close at 18:30 UK (13:30 ET)\n",
            "Brent, WTI, Midland WTI AGC and Permian WTI Storage...\n",
            "Found key D at line: D Trading Hours\n",
            "Extracted key D with content: Trading Hours\n",
            "Early close at 18:30 UK (13:30 ET)\n",
            "Brent, WTI, Midland WTI AGC and Permian WTI Storage...\n",
            "Extracted keys: ['A', 'B', 'C', 'D']\n",
            "Data saved to trading_schedule_key.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import json\n",
        "import re\n",
        "\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "df = pd.read_csv(\"cleaned_data.csv\")\n",
        "df['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\", errors='coerce')\n",
        "df = df.dropna(subset=['Date'])\n",
        "\n",
        "with open(\"/content/trading_schedule_key.json\", \"r\") as f:\n",
        "    status_info = json.load(f)\n",
        "\n",
        "def parse_sections(text):\n",
        "    commodities = []\n",
        "    times = []\n",
        "    current_section = None\n",
        "\n",
        "    # Split text into lines and process\n",
        "    for line in text.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if line in [\"Settlement Times\", \"Marker Times\"]:\n",
        "            current_section = line\n",
        "            continue\n",
        "        if current_section == \"Settlement Times\":\n",
        "            # Match standard settlement time pattern\n",
        "            if re.match(r\"Designated settlement period \\d{2}:\\d{2}\\s*-\\s*\\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*–\\s*\\d{2}:\\d{2}\\s*ET\\)\", line):\n",
        "                time = re.search(r\"(\\d{2}:\\d{2}\\s*-\\s*\\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*–\\s*\\d{2}:\\d{2}\\s*ET\\))\", line).group(1)\n",
        "                times.append(time)\n",
        "                commodities.append(\"\")  # Placeholder for commodity\n",
        "            # Match White Sugar settlement time\n",
        "            elif re.match(r\"White Sugar Futures & Options Designated settlement period \\d{2}:\\d{2}-\\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*–\\s*\\d{2}:\\d{2}\\s*ET\\)\", line):\n",
        "                time = re.search(r\"(\\d{2}:\\d{2}-\\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*–\\s*\\d{2}:\\d{2}\\s*ET\\))\", line).group(1)\n",
        "                times.append(time)\n",
        "                commodities.append(\"White Sugar Futures & Options\")\n",
        "            # Match commodity lines\n",
        "            elif not line.startswith(\"**\") and commodities and commodities[-1] == \"\":\n",
        "                commodities[-1] = line  # Update placeholder with commodity\n",
        "        elif current_section == \"Marker Times\":\n",
        "            # Match Low Sulphur Gasoil marker time\n",
        "            if re.match(r\"Low Sulphur Gasoil Futures US Minute Marker calculated at \\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*ET\\)\", line):\n",
        "                time = re.search(r\"(\\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*ET\\))\", line).group(1)\n",
        "                times.append(time)\n",
        "                commodities.append(\"Low Sulphur Gasoil Futures\")\n",
        "\n",
        "    # Split grouped commodities\n",
        "    final_commodities = []\n",
        "    final_times = []\n",
        "    for comm, time in zip(commodities, times):\n",
        "        if comm:  # Skip empty commodities\n",
        "            # Split commodities like \"Brent, WTI, Midland WTI AGC, Dubai Crude & Permian WTI Storage Futures & Options\"\n",
        "            comm_list = [c.strip() for c in comm.replace(\" & \", \", \").split(\", \")]\n",
        "            for c in comm_list:\n",
        "                final_commodities.append(c)\n",
        "                final_times.append(time)\n",
        "\n",
        "    # Create DataFrame\n",
        "    return pd.DataFrame({\n",
        "        \"Commodity\": final_commodities,\n",
        "        \"Time\": final_times\n",
        "    })\n",
        "\n",
        "input_date_str = input(\"Enter a date (dd/mm/yyyy): \")\n",
        "\n",
        "try:\n",
        "    input_date = datetime.strptime(input_date_str, \"%d/%m/%Y\")\n",
        "    df['Date_Diff'] = df['Date'].apply(lambda x: abs((x - input_date).days))\n",
        "    closest_row = df.loc[df['Date_Diff'].idxmin()].drop('Date_Diff')\n",
        "\n",
        "    print(\"\\nClosest Matching Date Entry:\\n\")\n",
        "    print(closest_row)\n",
        "\n",
        "    status_value = str(closest_row.get('Status', '')).strip().upper()\n",
        "    if status_value in status_info:\n",
        "        print(f\"\\nSettlement Times for status '{status_value}':\\n\")\n",
        "        print(parse_sections(status_info[status_value]).to_string(index=False))\n",
        "    else:\n",
        "        print(\"\\nNo mapped info found for status:\", status_value)\n",
        "\n",
        "except ValueError:\n",
        "    print(\"Invalid date format. Please enter the date in dd/mm/yyyy format.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bS51KAFtqKo",
        "outputId": "d4714230-55fb-4ff1-d734-5ed71ec66592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a date (dd/mm/yyyy): 11/11/2025\n",
            "\n",
            "Closest Matching Date Entry:\n",
            "\n",
            "Date                                                                          2025-11-27 00:00:00\n",
            "Occasion                                                                 Thanksgiving\\nUS Holiday\n",
            "Status                                                                                          C\n",
            "Additional Information    For further details, please refer to Trading Schedule Key\\non pages 4-5\n",
            "Name: 13, dtype: object\n",
            "\n",
            "Settlement Times for status 'C':\n",
            "\n",
            "Commodity                   Time                              \n",
            "                      Brent 17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "                        WTI 17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "            Midland WTI AGC 17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "                Dubai Crude 17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "Permian WTI Storage Futures 17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "                    Options 17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "        White Sugar Futures  16:53-16:55 UK (11:53 – 11:55 ET)\n",
            "                    Options  16:53-16:55 UK (11:53 – 11:55 ET)\n",
            " Low Sulphur Gasoil Futures                18:00 UK (13:00 ET)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Set pandas display options for better formatting\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "pd.set_option('display.colheader_justify', 'left')\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "\n",
        "# Read the CSV and clean the data\n",
        "df = pd.read_csv(\"cleaned_data.csv\")\n",
        "df['Date'] = pd.to_datetime(df['Date'], format=\"%d/%m/%Y\", errors='coerce')\n",
        "df = df.dropna(subset=['Date'])\n",
        "\n",
        "# Load the trading schedule JSON\n",
        "with open(\"trading_schedule_key.json\", \"r\") as f:\n",
        "    status_info = json.load(f)\n",
        "\n",
        "def parse_sections(text):\n",
        "    commodities = []\n",
        "    times = []\n",
        "    current_section = None\n",
        "\n",
        "    for line in text.splitlines():\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if line in [\"Settlement Times\", \"Marker Times\"]:\n",
        "            current_section = line\n",
        "            continue\n",
        "        if current_section == \"Settlement Times\":\n",
        "            if re.match(r\"Designated settlement period \\d{2}:\\d{2}\\s*-\\s*\\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*–\\s*\\d{2}:\\d{2}\\s*ET\\)\", line):\n",
        "                time = re.search(r\"(\\d{2}:\\d{2}\\s*-\\s*\\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*–\\s*\\d{2}:\\d{2}\\s*ET\\))\", line).group(1)\n",
        "                times.append(time)\n",
        "                commodities.append(\"\")\n",
        "            elif re.match(r\"White Sugar Futures & Options Designated settlement period \\d{2}:\\d{2}-\\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*–\\s*\\d{2}:\\d{2}\\s*ET\\)\", line):\n",
        "                time = re.search(r\"(\\d{2}:\\d{2}-\\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*–\\s*\\d{2}:\\d{2}\\s*ET\\))\", line).group(1)\n",
        "                times.append(time)\n",
        "                commodities.append(\"White Sugar Futures & Options\")\n",
        "            elif not line.startswith(\"**\") and commodities and commodities[-1] == \"\":\n",
        "                commodities[-1] = line\n",
        "        elif current_section == \"Marker Times\":\n",
        "            if re.match(r\"Low Sulphur Gasoil Futures US Minute Marker calculated at \\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*ET\\)\", line):\n",
        "                time = re.search(r\"(\\d{2}:\\d{2}\\s*UK\\s*\\(\\d{2}:\\d{2}\\s*ET\\))\", line).group(1)\n",
        "                times.append(time)\n",
        "                commodities.append(\"Low Sulphur Gasoil Futures\")\n",
        "\n",
        "    final_commodities = []\n",
        "    final_times = []\n",
        "    for comm, time in zip(commodities, times):\n",
        "        if comm:\n",
        "            comm_list = [c.strip() for c in comm.replace(\" & \", \", \").split(\", \")]\n",
        "            for c in comm_list:\n",
        "                final_commodities.append(c)\n",
        "                final_times.append(time)\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"Commodity\": final_commodities,\n",
        "        \"Settlement Time\": final_times\n",
        "    })\n",
        "\n",
        "def format_table(df, col_widths):\n",
        "    \"\"\"Custom function to format table with left-aligned content.\"\"\"\n",
        "    columns = df.columns\n",
        "    data = df.values\n",
        "    col_widths = col_widths or {col: max(len(str(col)), max(len(str(x)) for x in df[col])) for col in columns}\n",
        "\n",
        "    # Format header (left-aligned)\n",
        "    header = \"  \".join(f\"{col:<{col_widths[col]}}\" for col in columns)\n",
        "    # Format rows (left-aligned)\n",
        "    rows = [\n",
        "        \"  \".join(f\"{str(val):<{col_widths[col]}}\" for val, col in zip(row, columns))\n",
        "        for row in data\n",
        "    ]\n",
        "    # Combine header and rows\n",
        "    return \"\\n\".join([header] + rows)\n",
        "\n",
        "# Get user input\n",
        "input_date_str = input(\"Enter a date (dd/mm/yyyy): \")\n",
        "\n",
        "try:\n",
        "    input_date = datetime.strptime(input_date_str, \"%d/%m/%Y\")\n",
        "    df['Date_Diff'] = df['Date'].apply(lambda x: abs((x - input_date).days))\n",
        "    closest_row = df.loc[df['Date_Diff'].idxmin()].copy()\n",
        "    closest_row = closest_row.drop('Date_Diff')\n",
        "\n",
        "    # Clean up newline characters in the row\n",
        "    for col in closest_row.index:\n",
        "        if isinstance(closest_row[col], str):\n",
        "            closest_row[col] = closest_row[col].replace('\\n', ' ').strip()\n",
        "\n",
        "    # Format the output\n",
        "    print(\"\\nClosest Matching Date Entry:\\n\")\n",
        "    print(\"Date:               \", closest_row['Date'].strftime(\"%d/%m/%Y\"))\n",
        "    print(\"Occasion:           \", closest_row.get('Occasion', 'N/A'))\n",
        "    print(\"Status:             \", closest_row.get('Status', 'N/A'))\n",
        "    print(\"Additional Info:    \", closest_row.get('Additional Information', 'N/A'))\n",
        "\n",
        "    # Process settlement times\n",
        "    status_value = str(closest_row.get('Status', '')).strip().upper()\n",
        "    if status_value in status_info:\n",
        "        print(f\"\\nSettlement Times for Status '{status_value}':\\n\")\n",
        "        settlement_df = parse_sections(status_info[status_value])\n",
        "        print(format_table(settlement_df, col_widths={'Commodity': 30, 'Settlement Time': 30}))\n",
        "    else:\n",
        "        print(\"\\nNo mapped info found for status:\", status_value)\n",
        "\n",
        "except ValueError as e:\n",
        "    print(f\"Invalid date format. Please enter the date in dd/mm/yyyy format (e.g., 01/02/2025). Error: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OCn8b3e8fCq",
        "outputId": "f3d7ffe9-d181-4fea-e285-cad5617f284d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a date (dd/mm/yyyy): 11/11/2025\n",
            "\n",
            "Closest Matching Date Entry:\n",
            "\n",
            "Date:                27/11/2025\n",
            "Occasion:            Thanksgiving US Holiday\n",
            "Status:              C\n",
            "Additional Info:     For further details, please refer to Trading Schedule Key on pages 4-5\n",
            "\n",
            "Settlement Times for Status 'C':\n",
            "\n",
            "Commodity                       Settlement Time               \n",
            "Brent                           17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "WTI                             17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "Midland WTI AGC                 17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "Dubai Crude                     17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "Permian WTI Storage Futures     17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "Options                         17:58 -18:00 UK (12:58 – 13:00 ET)\n",
            "White Sugar Futures             16:53-16:55 UK (11:53 – 11:55 ET)\n",
            "Options                         16:53-16:55 UK (11:53 – 11:55 ET)\n",
            "Low Sulphur Gasoil Futures      18:00 UK (13:00 ET)           \n"
          ]
        }
      ]
    }
  ]
}